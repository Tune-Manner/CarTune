Index: ../backend/api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from fastapi import FastAPI, File, UploadFile\r\nimport speech_recognition as sr\r\nfrom transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\r\nfrom tensorflow.python.keras.models import load_model\r\nfrom tensorflow import keras\r\nfrom keras._tf_keras.keras.layers import BatchNormalization\r\nfrom keras._tf_keras.keras.preprocessing.image import smart_resize, load_img, img_to_array\r\nfrom PIL import Image\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport numpy as np\r\nimport io\r\nimport pprint\r\n\r\n# FastAPI 설정\r\napp = FastAPI()\r\n\r\n#CORS 설정\r\norigins = [\r\n    \"http://localhost:3000\"\r\n]\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=origins,\r\n    allow_credentials=True,\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n# KoBART 모델과 토크나이저 로드\r\nkobart_model_name = 'hyunwoongko/kobart'\r\nkobart_model = BartForConditionalGeneration.from_pretrained(kobart_model_name)\r\nkobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(kobart_model_name)\r\n\r\n# TensorFlow 모델 로드\r\nweather_model = load_model(\"models/trainedModelE10.h5\")\r\ncustom_objects = {'BatchNormalization': BatchNormalization}\r\n\r\n# 모델 입력 형상 확인\r\ninput_shape = weather_model.input_shape[1:]\r\n\r\n# KoBART 텍스트 생성 함수\r\ndef generate_answer(input_text, max_length=50):\r\n    input_ids = kobart_tokenizer.encode(input_text, return_tensors='pt')\r\n    output_ids = kobart_model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\r\n    output_text = kobart_tokenizer.decode(output_ids[0], skip_special_tokens=True)\r\n    return output_text\r\n\r\n# KoBART 의도 및 엔티티 추출 함수\r\ndef extract_intent_entity(input_text):\r\n    intents = [\"음악\", \"추천\", \"듣기\", \"찾기\", \"검색\"]\r\n    entities = [\"장르\", \"시간\", \"장소\", \"가수\", \"노래\", \"앨범\", \"발매일\", \"차트\", \"인기\", \"신곡\", \"제목\"]\r\n    unnecessary_words = [\r\n        \"추천\", \"해줘\", \"부탁해\", \"주세요\", \"제발\", \"고마워\", \"감사합니다\", \"해\", \"줘\",\r\n        \"좀\", \"할래\", \"줘요\", \"가\", \"입니다\", \"과\", \"그리고\", \"또는\", \"는\", \"이\", \"가\", \"를\", \"을\", \"에\", \"의\", \"와\", \"과\", \"에서\", \"로\", \"하다\"\r\n    ]\r\n\r\n    words = input_text.split()\r\n    filtered_words = [word for word in words if word not in unnecessary_words]\r\n    filtered_text = \" \".join(filtered_words)\r\n\r\n    detected_intent = None\r\n    detected_entities = []\r\n\r\n    for intent in intents:\r\n        if intent in filtered_text:\r\n            detected_intent = intent\r\n            break\r\n\r\n    if detected_intent is None:\r\n        if \"?\" in filtered_text:\r\n            detected_intent = \"질문\"\r\n        else:\r\n            detected_intent = \"명령\"\r\n\r\n    for entity in entities:\r\n        if entity in filtered_text:\r\n            detected_entities.append(entity)\r\n    \r\n    keywords = filtered_text.split()\r\n    for keyword in keywords:\r\n        if keyword not in detected_entities and keyword not in intents:\r\n            detected_entities.append(keyword)\r\n\r\n    return detected_intent, detected_entities\r\n\r\n@app.post(\"/transcribe/\")\r\nasync def transcribe_audio():\r\n    recognizer = sr.Recognizer()\r\n    try:\r\n        with sr.Microphone() as source:\r\n            print(\"말씀하세요...\")\r\n            recognizer.adjust_for_ambient_noise(source)\r\n            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\r\n\r\n        text = recognizer.recognize_google(audio, language='ko-KR')\r\n        print(\"text:\" + text)\r\n\r\n        # 여기에 generate_answer 및 extract_intent_entity 함수를 추가\r\n        answer = generate_answer(text)\r\n        intent, entities = extract_intent_entity(text)\r\n\r\n        return {\r\n            \"text\": text,\r\n            \"answer\": answer,\r\n            \"intent\": intent,\r\n            \"entities\": entities\r\n        }\r\n    except sr.UnknownValueError:\r\n        return {\"error\": \"Google 음성 인식이 오디오를 이해하지 못했습니다.\"}\r\n    except sr.RequestError as e:\r\n        return {\"error\": f\"Google 음성 인식 서비스 요청에 실패했습니다; {e}\"}\r\n\r\n# 이미지 전처리 함수\r\ndef preprocess_image(image: Image.Image) -> np.ndarray:\r\n    image = image.resize((input_shape[0], input_shape[1]))  # 모델에 맞게 이미지 크기 조정\r\n    image = np.array(image) / 255.0  # 이미지 정규화\r\n    image = np.expand_dims(image, axis=0)  # 배치 차원 추가\r\n    return image\r\n\r\n# 이미지 예측 함수\r\ndef predict_image(image: np.ndarray) -> int:\r\n    predictions = weather_model.predict(image)\r\n    pprint.pprint(predictions)\r\n    predicted_class = np.argmax(predictions, axis=1)[0]\r\n    return predicted_class\r\n\r\n# 날씨 예측 엔드포인트\r\n@app.post(\"/weather-predict/\")\r\nasync def predict(file: UploadFile = File(...)):\r\n    # 이미지 읽기 및 전처리\r\n    image_bytes = await file.read()\r\n    image = Image.open(io.BytesIO(image_bytes))\r\n    processed_image = preprocess_image(image)\r\n    \r\n    # 모델 예측\r\n    predicted_class = predict_image(processed_image)\r\n    \r\n    # 예측 결과 반환\r\n    return {\r\n        \"predicted_class\": int(predicted_class)\r\n    }\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/../backend/api.py b/../backend/api.py
--- a/../backend/api.py	
+++ b/../backend/api.py	
@@ -99,6 +99,7 @@
         answer = generate_answer(text)
         intent, entities = extract_intent_entity(text)
 
+        print("키워드",entities)
         return {
             "text": text,
             "answer": answer,
Index: ../backend/api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from fastapi import FastAPI, File, UploadFile\r\nimport speech_recognition as sr\r\nfrom transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\r\nfrom tensorflow.python.keras.models import load_model\r\nfrom tensorflow import keras\r\nfrom keras._tf_keras.keras.layers import BatchNormalization\r\nfrom keras._tf_keras.keras.preprocessing.image import smart_resize, load_img, img_to_array\r\nfrom PIL import Image\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nimport numpy as np\r\nimport io\r\nimport pprint\r\n\r\n# FastAPI 설정\r\napp = FastAPI()\r\n\r\n#CORS 설정\r\norigins = [\r\n    \"http://localhost:3000\"\r\n]\r\n\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=origins,\r\n    allow_credentials=True,\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n# KoBART 모델과 토크나이저 로드\r\nkobart_model_name = 'hyunwoongko/kobart'\r\nkobart_model = BartForConditionalGeneration.from_pretrained(kobart_model_name)\r\nkobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(kobart_model_name)\r\n\r\n# TensorFlow 모델 로드\r\nweather_model = load_model(\"models/trainedModelE10.h5\")\r\ncustom_objects = {'BatchNormalization': BatchNormalization}\r\n\r\n# 모델 입력 형상 확인\r\ninput_shape = weather_model.input_shape[1:]\r\n\r\n# KoBART 텍스트 생성 함수\r\ndef generate_answer(input_text, max_length=50):\r\n    input_ids = kobart_tokenizer.encode(input_text, return_tensors='pt')\r\n    output_ids = kobart_model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\r\n    output_text = kobart_tokenizer.decode(output_ids[0], skip_special_tokens=True)\r\n    return output_text\r\n\r\n# KoBART 의도 및 엔티티 추출 함수\r\ndef extract_intent_entity(input_text):\r\n    intents = [\"음악\", \"추천\", \"듣기\", \"찾기\", \"검색\"]\r\n    entities = [\"장르\", \"시간\", \"장소\", \"가수\", \"노래\", \"앨범\", \"발매일\", \"차트\", \"인기\", \"신곡\", \"제목\"]\r\n    unnecessary_words = [\r\n        \"추천\", \"해줘\", \"부탁해\", \"주세요\", \"제발\", \"고마워\", \"감사합니다\", \"해\", \"줘\",\r\n        \"좀\", \"할래\", \"줘요\", \"가\", \"입니다\", \"과\", \"그리고\", \"또는\", \"는\", \"이\", \"가\", \"를\", \"을\", \"에\", \"의\", \"와\", \"과\", \"에서\", \"로\", \"하다\"\r\n    ]\r\n\r\n    words = input_text.split()\r\n    filtered_words = [word for word in words if word not in unnecessary_words]\r\n    filtered_text = \" \".join(filtered_words)\r\n\r\n    detected_intent = None\r\n    detected_entities = []\r\n\r\n    for intent in intents:\r\n        if intent in filtered_text:\r\n            detected_intent = intent\r\n            break\r\n\r\n    if detected_intent is None:\r\n        if \"?\" in filtered_text:\r\n            detected_intent = \"질문\"\r\n        else:\r\n            detected_intent = \"명령\"\r\n\r\n    for entity in entities:\r\n        if entity in filtered_text:\r\n            detected_entities.append(entity)\r\n    \r\n    keywords = filtered_text.split()\r\n    for keyword in keywords:\r\n        if keyword not in detected_entities and keyword not in intents:\r\n            detected_entities.append(keyword)\r\n\r\n    return detected_intent, detected_entities\r\n\r\n@app.post(\"/transcribe/\")\r\nasync def transcribe_audio():\r\n    recognizer = sr.Recognizer()\r\n    try:\r\n        with sr.Microphone() as source:\r\n            print(\"말씀하세요...\")\r\n            recognizer.adjust_for_ambient_noise(source)\r\n            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\r\n\r\n        text = recognizer.recognize_google(audio, language='ko-KR')\r\n        print(\"text:\" + text)\r\n\r\n        # 여기에 generate_answer 및 extract_intent_entity 함수를 추가\r\n        answer = generate_answer(text)\r\n        intent, entities = extract_intent_entity(text)\r\n\r\n        return {\r\n            \"text\": text,\r\n            \"answer\": answer,\r\n            \"intent\": intent,\r\n            \"entities\": entities\r\n        }\r\n    except sr.UnknownValueError:\r\n        return {\"error\": \"Google 음성 인식이 오디오를 이해하지 못했습니다.\"}\r\n    except sr.RequestError as e:\r\n        return {\"error\": f\"Google 음성 인식 서비스 요청에 실패했습니다; {e}\"}\r\n\r\n# 이미지 전처리 함수\r\ndef preprocess_image(image: Image.Image) -> np.ndarray:\r\n    image = image.resize((input_shape[0], input_shape[1]))  # 모델에 맞게 이미지 크기 조정\r\n    image = np.array(image) / 255.0  # 이미지 정규화\r\n    image = np.expand_dims(image, axis=0)  # 배치 차원 추가\r\n    return image\r\n\r\n# 이미지 예측 함수\r\ndef predict_image(image: np.ndarray) -> int:\r\n    predictions = weather_model.predict(image)\r\n    pprint.pprint(predictions)\r\n    predicted_class = np.argmax(predictions, axis=1)[0]\r\n    return predicted_class\r\n\r\n# 날씨 예측 엔드포인트\r\n@app.post(\"/weather-predict/\")\r\nasync def predict(file: UploadFile = File(...)):\r\n    # 이미지 읽기 및 전처리\r\n    image_bytes = await file.read()\r\n    image = Image.open(io.BytesIO(image_bytes))\r\n    processed_image = preprocess_image(image)\r\n    \r\n    # 모델 예측\r\n    predicted_class = predict_image(processed_image)\r\n    \r\n    # 예측 결과 반환\r\n    return {\r\n        \"predicted_class\": int(predicted_class)\r\n    }\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/../backend/api.py b/../backend/api.py
--- a/../backend/api.py	
+++ b/../backend/api.py	
@@ -99,6 +99,7 @@
         answer = generate_answer(text)
         intent, entities = extract_intent_entity(text)
 
+        print("키워드",entities)
         return {
             "text": text,
             "answer": answer,
